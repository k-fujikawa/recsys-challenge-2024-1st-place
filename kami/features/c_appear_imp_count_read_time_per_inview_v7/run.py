"""
1h/24h/期間全体での impression に対して、同じarticleが何回出現するかをカウントする
user・sessionごとに、過去・未来それぞれでのカウントを行う
また、その値を同じimpression内での割合に変換した特徴量も組み込む
"""

import itertools
import os
import sys
from pathlib import Path

import hydra
import polars as pl
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig
from tqdm.auto import tqdm

from utils.data import get_data_dirs

PREFIX = "c"

FIX_WINDOWS = []  # ["1i", "2i", "3i", "5i"]  #
USE_TIMES = ["5m", "1h"]  # ["5m"]  #

BASE_COLUMNS = list(
    itertools.chain(
        *[
            [
                f"{group_name}_read_time_sum_past_all",
                f"{group_name}_read_time_sum_future_all",
                f"{group_name}_read_time_sum_past_and_future_all",
            ]
            + [
                f"{group_name}_read_time_sum_past_{time_str}"
                for time_str in USE_TIMES + FIX_WINDOWS
            ]
            + [
                f"{group_name}_read_time_sum_future_{time_str}"
                for time_str in USE_TIMES + FIX_WINDOWS
            ]
            + [
                f"{group_name}_read_time_sum_past_and_future_{time_str}"
                for time_str in USE_TIMES + FIX_WINDOWS
            ]
            for group_name in ["user", "common"]  # "session"
        ]
    )
)

USE_COLUMNS = (
    [col + "_ratio" for col in BASE_COLUMNS]
    + [col + "_rank_ascending" for col in BASE_COLUMNS]
    + [col + "_rank_descending" for col in BASE_COLUMNS]
)


def process_df(cfg, behaviors_df):
    behaviors_df = behaviors_df.select(
        [
            "impression_id",
            "impression_time",
            "article_ids_inview",
            "session_id",
            "user_id",
            "read_time",
            "scroll_percentage",
        ]
    ).with_columns(
        (
            pl.when(pl.col("scroll_percentage").is_null())
            .then(0)
            .otherwise(pl.col("read_time"))
            / pl.col("article_ids_inview").list.len()
        ).alias("read_time_per_inview")
    )
    explode_df = (
        behaviors_df.explode("article_ids_inview")
        .rename({"article_ids_inview": "article_id"})
        .with_row_index(name="order")
    )  # 後で戻せるように番号付与

    # ソートして後で使うカラムを付与
    explode_df = explode_df.sort(["impression_time"]).with_columns(
        [
            # 逆順にrollingしたいので、時間を逆順にする
            (
                pl.col("impression_time")
                - (pl.col("impression_time") - pl.col("impression_time").min())
                - (pl.col("impression_time") - pl.col("impression_time").min())
            )
            .dt.replace_time_zone(time_zone=None)
            .alias("reverse_impression_time"),
        ]
    )

    for group_name, group_subset in [
        # ["session", ["user_id", "session_id", "article_id"]],
        ["user", ["user_id", "article_id"]],
        ["common", ["article_id"]],
    ]:
        print(f"processing {group_name}")
        explode_df = explode_df.with_columns(
            [
                (pl.col("read_time_per_inview").cum_sum().over(group_subset) - 1).alias(
                    f"{group_name}_read_time_sum_past_all"
                ),
                (
                    pl.col("read_time_per_inview")
                    .cum_sum(reverse=True)
                    .over(group_subset)
                    - 1
                ).alias(f"{group_name}_read_time_sum_future_all"),
            ]
        )

        print("past")
        explode_df = explode_df.with_columns(
            [
                (
                    pl.col("read_time_per_inview")
                    .rolling_sum(window_size=time_str, by="impression_time")
                    .over(group_subset)
                    - 1
                ).alias(f"{group_name}_read_time_sum_past_{time_str}")
                for time_str in USE_TIMES
            ]
            + [
                (
                    pl.col("read_time_per_inview")
                    .rolling_sum(window_size=time_str)
                    .over(group_subset)
                    - 1
                ).alias(f"{group_name}_read_time_sum_past_{time_str}")
                for time_str in FIX_WINDOWS
            ]
        )

        print("future")
        explode_df = explode_df.reverse()
        explode_df = explode_df.with_columns(
            [
                (
                    pl.col("read_time_per_inview")
                    .rolling_sum(window_size=time_str, by="reverse_impression_time")
                    .over(group_subset)
                    - 1
                ).alias(f"{group_name}_read_time_sum_future_{time_str}")
                for time_str in USE_TIMES
            ]
            + [
                (
                    pl.col("read_time_per_inview")
                    .rolling_sum(window_size=time_str)
                    .over(group_subset)
                    - 1
                ).alias(f"{group_name}_read_time_sum_future_{time_str}")
                for time_str in FIX_WINDOWS
            ]
        )

        # 逆順にしたので元に戻す
        explode_df = explode_df.reverse()

        # past と futer を合わせる
        explode_df = explode_df.with_columns(
            [
                (
                    pl.col(f"{group_name}_read_time_sum_past_all")
                    + pl.col(f"{group_name}_read_time_sum_future_all")
                ).alias(f"{group_name}_read_time_sum_past_and_future_all"),
            ]
            + [
                (
                    pl.col(f"{group_name}_read_time_sum_past_{time_str}")
                    + pl.col(f"{group_name}_read_time_sum_future_{time_str}")
                ).alias(f"{group_name}_read_time_sum_past_and_future_{time_str}")
                for time_str in USE_TIMES + FIX_WINDOWS
            ]
        )

    # 同一 impression 内での割合,rankを求める
    for col in BASE_COLUMNS:
        explode_df = explode_df.with_columns(
            [
                (pl.col(col) / pl.col(col).sum().over("impression_id")).alias(
                    f"{col}_ratio"
                ),
                pl.col(col).rank().over("impression_id").alias(f"{col}_rank_ascending"),
                pl.col(col)
                .rank(descending=True)
                .over("impression_id")
                .alias(f"{col}_rank_descending"),
            ]
        )
    explode_df = explode_df.sort(["order"])
    return explode_df


def create_feature(cfg: DictConfig, output_path):
    input_dir = Path(cfg.dir.input_dir)
    size_name = cfg.exp.size_name
    data_dirs = get_data_dirs(input_dir, size_name)

    for data_name in ["train", "validation", "test"]:
        print(f"processing {data_name} data")
        behaviors_df = pl.read_parquet(data_dirs[data_name] / "behaviors.parquet")
        df = process_df(cfg, behaviors_df).select(USE_COLUMNS)

        df = df.rename({col: f"{PREFIX}_{col}" for col in USE_COLUMNS})
        print(df)
        df.write_parquet(
            output_path / f"{data_name}_feat.parquet",
        )


@hydra.main(version_base=None, config_path=".", config_name="config")
def main(cfg: DictConfig) -> None:
    runtime_choices = HydraConfig.get().runtime.choices
    exp_name = f"{Path(sys.argv[0]).parent.name}/{runtime_choices.exp}"

    print(f"exp_name: {exp_name}")
    output_path = Path(cfg.dir.features_dir) / exp_name
    print(f"ouput_path: {output_path}")
    os.makedirs(output_path, exist_ok=True)

    create_feature(cfg, output_path)


if __name__ == "__main__":
    main()
